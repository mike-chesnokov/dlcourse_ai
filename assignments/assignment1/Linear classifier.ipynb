{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:50:19.691142Z",
     "start_time": "2019-06-12T19:50:19.572909Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:50:20.232328Z",
     "start_time": "2019-06-12T19:50:20.189152Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:50:23.216053Z",
     "start_time": "2019-06-12T19:50:21.186647Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:50:23.318444Z",
     "start_time": "2019-06-12T19:50:23.307705Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:50:24.530820Z",
     "start_time": "2019-06-12T19:50:24.520949Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:30:54.252371Z",
     "start_time": "2019-06-12T19:30:54.236394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:50:26.891460Z",
     "start_time": "2019-06-12T19:50:26.881745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:30:55.955393Z",
     "start_time": "2019-06-12T19:30:55.937613Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5514447139320511, array([-0.42388312,  0.21194156,  0.21194156]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:30:56.414115Z",
     "start_time": "2019-06-12T19:30:56.402667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57611688, 0.21194156, 0.21194156])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_classifer.softmax(np.array([1, 0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:30:56.783712Z",
     "start_time": "2019-06-12T19:30:56.767124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.55144471, -1.55144471, -1.55144471])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(linear_classifer.softmax(np.array([1, 0, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:50:28.236129Z",
     "start_time": "2019-06-12T19:50:28.224697Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 3\n",
    "batch_size = 2\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "loss, grads = linear_classifer.softmax_with_cross_entropy(predictions, target_index)\n",
    "\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:50:29.779812Z",
     "start_time": "2019-06-12T19:50:29.764904Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.37025176982879043\n",
      "delta [[1.e-05 0.e+00 0.e+00]\n",
      " [0.e+00 0.e+00 0.e+00]] \n",
      " [[ 1.00001  2.      -1.     ]\n",
      " [ 1.       1.       2.     ]]\n",
      "temp1 1.4502247628372245 \n",
      "temp2 1.4502321678726209\n",
      "-0.37025176982119484 \n",
      "\n",
      "0.3526922563491206\n",
      "delta [[0.e+00 1.e-05 0.e+00]\n",
      " [0.e+00 0.e+00 0.e+00]] \n",
      " [[ 1.       2.00001 -1.     ]\n",
      " [ 1.       1.       2.     ]]\n",
      "temp1 1.4502319922778777 \n",
      "temp2 1.4502249384327506\n",
      "0.35269225635570217 \n",
      "\n",
      "0.017559513479669865\n",
      "delta [[0.e+00 0.e+00 1.e-05]\n",
      " [0.e+00 0.e+00 0.e+00]] \n",
      " [[ 1.       2.      -0.99999]\n",
      " [ 1.       1.       2.     ]]\n",
      "temp1 1.4502286409461007 \n",
      "temp2 1.450228289755831\n",
      "0.01755951348769713 \n",
      "\n",
      "-0.3940292211914573\n",
      "delta [[0.e+00 0.e+00 0.e+00]\n",
      " [1.e-05 0.e+00 0.e+00]] \n",
      " [[ 1.       2.      -1.     ]\n",
      " [ 1.00001  1.       2.     ]]\n",
      "temp1 1.4502245250620822 \n",
      "temp2 1.4502324056465061\n",
      "-0.3940292211956908 \n",
      "\n",
      "0.10597077880854272\n",
      "delta [[0.e+00 0.e+00 0.e+00]\n",
      " [0.e+00 1.e-05 0.e+00]] \n",
      " [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.00001  2.     ]]\n",
      "temp1 1.4502295250620825 \n",
      "temp2 1.450227405646506\n",
      "0.10597077881868698 \n",
      "\n",
      "0.28805844238291456\n",
      "delta [[0.e+00 0.e+00 0.e+00]\n",
      " [0.e+00 0.e+00 1.e-05]] \n",
      " [[ 1.       2.      -1.     ]\n",
      " [ 1.       1.       2.00001]]\n",
      "temp1 1.4502313459406477 \n",
      "temp2 1.4502255847718\n",
      "0.2880584423881061 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta=1e-5\n",
    "tol = 1e-4 \n",
    "z_ = predictions.copy()\n",
    "it = np.nditer(grads, flags=['multi_index'], op_flags=['readwrite'])\n",
    "\n",
    "while not it.finished:\n",
    "    ix = it.multi_index\n",
    "    print(grads[ix])\n",
    "    f = lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index)\n",
    "    \n",
    "    delta_vec = np.zeros_like(z_)\n",
    "    delta_vec[ix] += delta\n",
    "    print('delta', delta_vec, '\\n', z_ + delta_vec)\n",
    "    \n",
    "    temp1 = f(z_ + delta_vec)[0]\n",
    "    temp2 = f(z_ - delta_vec)[0]\n",
    "    print('temp1', temp1, '\\ntemp2', temp2)\n",
    "    print((temp1 - temp2)/(2 * delta), '\\n')\n",
    "    \n",
    "    it.iternext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:50:31.149678Z",
     "start_time": "2019-06-12T19:50:31.137076Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:50:33.109655Z",
     "start_time": "2019-06-12T19:50:33.101380Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:31:08.035577Z",
     "start_time": "2019-06-12T19:31:08.028317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1., -1.,  1.],\n",
       "       [ 0.,  1.,  1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:31:08.555968Z",
     "start_time": "2019-06-12T19:31:08.536123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.],\n",
       "       [-1.,  1.],\n",
       "       [ 1.,  2.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:50:39.895258Z",
     "start_time": "2019-06-12T19:50:39.887309Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1.],\n",
       "       [ 0.,  3.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.dot(X, W)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:50:40.223183Z",
     "start_time": "2019-06-12T19:50:40.215191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.44039854,  0.44039854],\n",
       "       [-0.4166856 ,  0.4166856 ],\n",
       "       [ 0.46411148, -0.46411148]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, grads = linear_classifer.linear_softmax(X, W, target_index)\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:50:40.852610Z",
     "start_time": "2019-06-12T19:50:40.828496Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.44039853898894116\n",
      "delta [[1.e-05 0.e+00]\n",
      " [0.e+00 0.e+00]\n",
      " [0.e+00 0.e+00]] \n",
      " [[ 1.00001  2.     ]\n",
      " [-1.       1.     ]\n",
      " [ 1.       2.     ]]\n",
      "temp1 1.0877532773255922 \n",
      "temp2 1.087762085296372\n",
      "-0.4403985389922482 \n",
      "\n",
      "-0.4166856024001578\n",
      "delta [[0.e+00 0.e+00]\n",
      " [1.e-05 0.e+00]\n",
      " [0.e+00 0.e+00]] \n",
      " [[ 1.       2.     ]\n",
      " [-0.99999  1.     ]\n",
      " [ 1.       2.     ]]\n",
      "temp1 1.0877535144560875 \n",
      "temp2 1.0877618481681357\n",
      "-0.4166856024112597 \n",
      "\n",
      "0.46411147557772453\n",
      "delta [[0.e+00 0.e+00]\n",
      " [0.e+00 0.e+00]\n",
      " [1.e-05 0.e+00]] \n",
      " [[ 1.       2.     ]\n",
      " [-1.       1.     ]\n",
      " [ 1.00001  2.     ]]\n",
      "temp1 1.0877623224268673 \n",
      "temp2 1.0877530401973559\n",
      "0.4641114755732367 \n",
      "\n",
      "0.4403985389889412\n",
      "delta [[0.e+00 1.e-05]\n",
      " [0.e+00 0.e+00]\n",
      " [0.e+00 0.e+00]] \n",
      " [[ 1.       2.00001]\n",
      " [-1.       1.     ]\n",
      " [ 1.       2.     ]]\n",
      "temp1 1.087762085296372 \n",
      "temp2 1.0877532773255922\n",
      "0.4403985389922482 \n",
      "\n",
      "0.4166856024001579\n",
      "delta [[0.e+00 0.e+00]\n",
      " [0.e+00 1.e-05]\n",
      " [0.e+00 0.e+00]] \n",
      " [[ 1.       2.     ]\n",
      " [-1.       1.00001]\n",
      " [ 1.       2.     ]]\n",
      "temp1 1.0877618481681357 \n",
      "temp2 1.0877535144560875\n",
      "0.4166856024112597 \n",
      "\n",
      "-0.46411147557772453\n",
      "delta [[0.e+00 0.e+00]\n",
      " [0.e+00 0.e+00]\n",
      " [0.e+00 1.e-05]] \n",
      " [[ 1.       2.     ]\n",
      " [-1.       1.     ]\n",
      " [ 1.       2.00001]]\n",
      "temp1 1.0877530401973559 \n",
      "temp2 1.0877623224268673\n",
      "-0.4641114755732367 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta=1e-5\n",
    "tol = 1e-4 \n",
    "z_ = W.copy()\n",
    "it = np.nditer(grads, flags=['multi_index'], op_flags=['readwrite'])\n",
    "\n",
    "while not it.finished:\n",
    "    ix = it.multi_index\n",
    "    print(grads[ix])\n",
    "    f = lambda w: linear_classifer.linear_softmax(X, w, target_index)\n",
    "    \n",
    "    delta_vec = np.zeros_like(z_)\n",
    "    delta_vec[ix] += delta\n",
    "    print('delta', delta_vec, '\\n', z_ + delta_vec)\n",
    "    \n",
    "    temp1 = f(z_ + delta_vec)[0]\n",
    "    temp2 = f(z_ - delta_vec)[0]\n",
    "    print('temp1', temp1, '\\ntemp2', temp2)\n",
    "    print((temp1 - temp2)/(2 * delta), '\\n')\n",
    "    \n",
    "    it.iternext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:50:43.914448Z",
     "start_time": "2019-06-12T19:50:43.903862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:50:44.632234Z",
     "start_time": "2019-06-12T19:50:44.620683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.44039854,  0.44039854],\n",
       "       [-0.4166856 ,  0.4166856 ],\n",
       "       [ 0.46411148, -0.46411148]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:50:46.201343Z",
     "start_time": "2019-06-12T19:50:46.186567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:50:48.536141Z",
     "start_time": "2019-06-12T19:50:47.391322Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.395944\n",
      "Epoch 1, loss: 2.329967\n",
      "Epoch 2, loss: 2.309746\n",
      "Epoch 3, loss: 2.303854\n",
      "Epoch 4, loss: 2.302685\n",
      "Epoch 5, loss: 2.303024\n",
      "Epoch 6, loss: 2.301804\n",
      "Epoch 7, loss: 2.302362\n",
      "Epoch 8, loss: 2.301462\n",
      "Epoch 9, loss: 2.301036\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:50:49.060372Z",
     "start_time": "2019-06-12T19:50:48.862476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:51:03.199436Z",
     "start_time": "2019-06-12T19:50:52.303307Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.119\n",
      "Epoch 0, loss: 2.301775\n",
      "Epoch 1, loss: 2.302470\n",
      "Epoch 2, loss: 2.301517\n",
      "Epoch 3, loss: 2.302304\n",
      "Epoch 4, loss: 2.301261\n",
      "Epoch 5, loss: 2.301431\n",
      "Epoch 6, loss: 2.302390\n",
      "Epoch 7, loss: 2.302629\n",
      "Epoch 8, loss: 2.301360\n",
      "Epoch 9, loss: 2.301406\n",
      "Epoch 10, loss: 2.301995\n",
      "Epoch 11, loss: 2.301792\n",
      "Epoch 12, loss: 2.302865\n",
      "Epoch 13, loss: 2.300573\n",
      "Epoch 14, loss: 2.301694\n",
      "Epoch 15, loss: 2.301212\n",
      "Epoch 16, loss: 2.302294\n",
      "Epoch 17, loss: 2.301142\n",
      "Epoch 18, loss: 2.301996\n",
      "Epoch 19, loss: 2.301716\n",
      "Epoch 20, loss: 2.301361\n",
      "Epoch 21, loss: 2.302726\n",
      "Epoch 22, loss: 2.301701\n",
      "Epoch 23, loss: 2.301551\n",
      "Epoch 24, loss: 2.302130\n",
      "Epoch 25, loss: 2.302418\n",
      "Epoch 26, loss: 2.301620\n",
      "Epoch 27, loss: 2.301936\n",
      "Epoch 28, loss: 2.301969\n",
      "Epoch 29, loss: 2.301542\n",
      "Epoch 30, loss: 2.301278\n",
      "Epoch 31, loss: 2.302039\n",
      "Epoch 32, loss: 2.302859\n",
      "Epoch 33, loss: 2.303249\n",
      "Epoch 34, loss: 2.301944\n",
      "Epoch 35, loss: 2.301846\n",
      "Epoch 36, loss: 2.302339\n",
      "Epoch 37, loss: 2.302446\n",
      "Epoch 38, loss: 2.301568\n",
      "Epoch 39, loss: 2.301545\n",
      "Epoch 40, loss: 2.301864\n",
      "Epoch 41, loss: 2.302947\n",
      "Epoch 42, loss: 2.302325\n",
      "Epoch 43, loss: 2.301754\n",
      "Epoch 44, loss: 2.301960\n",
      "Epoch 45, loss: 2.301942\n",
      "Epoch 46, loss: 2.301696\n",
      "Epoch 47, loss: 2.302248\n",
      "Epoch 48, loss: 2.301740\n",
      "Epoch 49, loss: 2.301784\n",
      "Epoch 50, loss: 2.301235\n",
      "Epoch 51, loss: 2.301655\n",
      "Epoch 52, loss: 2.302594\n",
      "Epoch 53, loss: 2.301782\n",
      "Epoch 54, loss: 2.302082\n",
      "Epoch 55, loss: 2.302244\n",
      "Epoch 56, loss: 2.302239\n",
      "Epoch 57, loss: 2.303319\n",
      "Epoch 58, loss: 2.301913\n",
      "Epoch 59, loss: 2.302222\n",
      "Epoch 60, loss: 2.301606\n",
      "Epoch 61, loss: 2.301208\n",
      "Epoch 62, loss: 2.301827\n",
      "Epoch 63, loss: 2.302937\n",
      "Epoch 64, loss: 2.302002\n",
      "Epoch 65, loss: 2.302777\n",
      "Epoch 66, loss: 2.302968\n",
      "Epoch 67, loss: 2.302866\n",
      "Epoch 68, loss: 2.302086\n",
      "Epoch 69, loss: 2.301090\n",
      "Epoch 70, loss: 2.301826\n",
      "Epoch 71, loss: 2.300684\n",
      "Epoch 72, loss: 2.302248\n",
      "Epoch 73, loss: 2.302451\n",
      "Epoch 74, loss: 2.301693\n",
      "Epoch 75, loss: 2.301706\n",
      "Epoch 76, loss: 2.301780\n",
      "Epoch 77, loss: 2.301653\n",
      "Epoch 78, loss: 2.301849\n",
      "Epoch 79, loss: 2.302842\n",
      "Epoch 80, loss: 2.302507\n",
      "Epoch 81, loss: 2.302186\n",
      "Epoch 82, loss: 2.301207\n",
      "Epoch 83, loss: 2.302617\n",
      "Epoch 84, loss: 2.302322\n",
      "Epoch 85, loss: 2.302050\n",
      "Epoch 86, loss: 2.301836\n",
      "Epoch 87, loss: 2.301914\n",
      "Epoch 88, loss: 2.302461\n",
      "Epoch 89, loss: 2.301592\n",
      "Epoch 90, loss: 2.302681\n",
      "Epoch 91, loss: 2.302394\n",
      "Epoch 92, loss: 2.302126\n",
      "Epoch 93, loss: 2.301533\n",
      "Epoch 94, loss: 2.302000\n",
      "Epoch 95, loss: 2.302418\n",
      "Epoch 96, loss: 2.302238\n",
      "Epoch 97, loss: 2.302749\n",
      "Epoch 98, loss: 2.301531\n",
      "Epoch 99, loss: 2.302757\n",
      "Accuracy after training for 100 epochs:  0.125\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:51:09.873459Z",
     "start_time": "2019-06-12T19:51:09.860342Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:58:48.793733Z",
     "start_time": "2019-06-12T19:58:28.197679Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.320629\n",
      "Epoch 1, loss: 2.318327\n",
      "Epoch 2, loss: 2.250892\n",
      "Epoch 3, loss: 2.320422\n",
      "Epoch 4, loss: 2.344472\n",
      "Epoch 5, loss: 2.302738\n",
      "Epoch 6, loss: 2.260803\n",
      "Epoch 7, loss: 2.337542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:01,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 2.370521\n",
      "Epoch 9, loss: 2.302183\n",
      "lr=0.1; reg=0.1\n",
      "Accuracy current:  0.194 \n",
      "\n",
      "Epoch 0, loss: 2.255254\n",
      "Epoch 1, loss: 2.243471\n",
      "Epoch 2, loss: 2.119944\n",
      "Epoch 3, loss: 2.210616\n",
      "Epoch 4, loss: 2.269949\n",
      "Epoch 5, loss: 2.230083\n",
      "Epoch 6, loss: 2.144227\n",
      "Epoch 7, loss: 2.257822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:02,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 2.316976\n",
      "Epoch 9, loss: 2.195097\n",
      "lr=0.1; reg=0.01\n",
      "Accuracy current:  0.231 \n",
      "\n",
      "Epoch 0, loss: 2.241433\n",
      "Epoch 1, loss: 2.219125\n",
      "Epoch 2, loss: 2.074873\n",
      "Epoch 3, loss: 2.164472\n",
      "Epoch 4, loss: 2.228727\n",
      "Epoch 5, loss: 2.182492\n",
      "Epoch 6, loss: 2.082369\n",
      "Epoch 7, loss: 2.199011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [00:03,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 2.266523\n",
      "Epoch 9, loss: 2.129244\n",
      "lr=0.1; reg=0.001\n",
      "Accuracy current:  0.243 \n",
      "\n",
      "Epoch 0, loss: 2.239910\n",
      "Epoch 1, loss: 2.216210\n",
      "Epoch 2, loss: 2.069259\n",
      "Epoch 3, loss: 2.158252\n",
      "Epoch 4, loss: 2.222558\n",
      "Epoch 5, loss: 2.174899\n",
      "Epoch 6, loss: 2.072488\n",
      "Epoch 7, loss: 2.188826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [00:04,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 2.256627\n",
      "Epoch 9, loss: 2.116913\n",
      "lr=0.1; reg=0.0001\n",
      "Accuracy current:  0.243 \n",
      "\n",
      "Epoch 0, loss: 2.290288\n",
      "Epoch 1, loss: 2.285788\n",
      "Epoch 2, loss: 2.242009\n",
      "Epoch 3, loss: 2.266892\n",
      "Epoch 4, loss: 2.268025\n",
      "Epoch 5, loss: 2.264731\n",
      "Epoch 6, loss: 2.242921\n",
      "Epoch 7, loss: 2.286863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [00:06,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 2.272821\n",
      "Epoch 9, loss: 2.257251\n",
      "lr=0.01; reg=0.1\n",
      "Accuracy current:  0.221 \n",
      "\n",
      "Epoch 0, loss: 2.284694\n",
      "Epoch 1, loss: 2.275619\n",
      "Epoch 2, loss: 2.217788\n",
      "Epoch 3, loss: 2.238871\n",
      "Epoch 4, loss: 2.235774\n",
      "Epoch 5, loss: 2.228667\n",
      "Epoch 6, loss: 2.188066\n",
      "Epoch 7, loss: 2.245489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [00:07,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 2.219175\n",
      "Epoch 9, loss: 2.189423\n",
      "lr=0.01; reg=0.01\n",
      "Accuracy current:  0.228 \n",
      "\n",
      "Epoch 0, loss: 2.284011\n",
      "Epoch 1, loss: 2.274233\n",
      "Epoch 2, loss: 2.214437\n",
      "Epoch 3, loss: 2.234558\n",
      "Epoch 4, loss: 2.230356\n",
      "Epoch 5, loss: 2.222087\n",
      "Epoch 6, loss: 2.178171\n",
      "Epoch 7, loss: 2.236695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [00:08,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 2.207737\n",
      "Epoch 9, loss: 2.174903\n",
      "lr=0.01; reg=0.001\n",
      "Accuracy current:  0.228 \n",
      "\n",
      "Epoch 0, loss: 2.283941\n",
      "Epoch 1, loss: 2.274090\n",
      "Epoch 2, loss: 2.214089\n",
      "Epoch 3, loss: 2.234105\n",
      "Epoch 4, loss: 2.229780\n",
      "Epoch 5, loss: 2.221380\n",
      "Epoch 6, loss: 2.177108\n",
      "Epoch 7, loss: 2.235730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [00:10,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 2.206479\n",
      "Epoch 9, loss: 2.173302\n",
      "lr=0.01; reg=0.0001\n",
      "Accuracy current:  0.228 \n",
      "\n",
      "Epoch 0, loss: 2.302513\n",
      "Epoch 1, loss: 2.301632\n",
      "Epoch 2, loss: 2.291294\n",
      "Epoch 3, loss: 2.294453\n",
      "Epoch 4, loss: 2.293231\n",
      "Epoch 5, loss: 2.294249\n",
      "Epoch 6, loss: 2.286263\n",
      "Epoch 7, loss: 2.288891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [00:11,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 2.288397\n",
      "Epoch 9, loss: 2.289214\n",
      "lr=0.001; reg=0.1\n",
      "Accuracy current:  0.184 \n",
      "\n",
      "Epoch 0, loss: 2.299759\n",
      "Epoch 1, loss: 2.298883\n",
      "Epoch 2, loss: 2.288258\n",
      "Epoch 3, loss: 2.291358\n",
      "Epoch 4, loss: 2.289984\n",
      "Epoch 5, loss: 2.290865\n",
      "Epoch 6, loss: 2.282095\n",
      "Epoch 7, loss: 2.284748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [00:12,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 2.283662\n",
      "Epoch 9, loss: 2.284324\n",
      "lr=0.001; reg=0.01\n",
      "Accuracy current:  0.185 \n",
      "\n",
      "Epoch 0, loss: 2.299474\n",
      "Epoch 1, loss: 2.298589\n",
      "Epoch 2, loss: 2.287924\n",
      "Epoch 3, loss: 2.291008\n",
      "Epoch 4, loss: 2.289608\n",
      "Epoch 5, loss: 2.290464\n",
      "Epoch 6, loss: 2.281602\n",
      "Epoch 7, loss: 2.284245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [00:14,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 2.283082\n",
      "Epoch 9, loss: 2.283715\n",
      "lr=0.001; reg=0.001\n",
      "Accuracy current:  0.186 \n",
      "\n",
      "Epoch 0, loss: 2.299445\n",
      "Epoch 1, loss: 2.298559\n",
      "Epoch 2, loss: 2.287890\n",
      "Epoch 3, loss: 2.290973\n",
      "Epoch 4, loss: 2.289570\n",
      "Epoch 5, loss: 2.290423\n",
      "Epoch 6, loss: 2.281551\n",
      "Epoch 7, loss: 2.284194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [00:15,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 2.283023\n",
      "Epoch 9, loss: 2.283653\n",
      "lr=0.001; reg=0.0001\n",
      "Accuracy current:  0.186 \n",
      "\n",
      "Epoch 0, loss: 2.306516\n",
      "Epoch 1, loss: 2.306047\n",
      "Epoch 2, loss: 2.302992\n",
      "Epoch 3, loss: 2.303013\n",
      "Epoch 4, loss: 2.304494\n",
      "Epoch 5, loss: 2.304244\n",
      "Epoch 6, loss: 2.302696\n",
      "Epoch 7, loss: 2.302279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [00:16,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 2.302158\n",
      "Epoch 9, loss: 2.303320\n",
      "lr=0.0001; reg=0.1\n",
      "Accuracy current:  0.132 \n",
      "\n",
      "Epoch 0, loss: 2.303722\n",
      "Epoch 1, loss: 2.303264\n",
      "Epoch 2, loss: 2.300207\n",
      "Epoch 3, loss: 2.300234\n",
      "Epoch 4, loss: 2.301734\n",
      "Epoch 5, loss: 2.301487\n",
      "Epoch 6, loss: 2.299932\n",
      "Epoch 7, loss: 2.299523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "14it [00:17,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 2.299394\n",
      "Epoch 9, loss: 2.300577\n",
      "lr=0.0001; reg=0.01\n",
      "Accuracy current:  0.133 \n",
      "\n",
      "Epoch 0, loss: 2.303442\n",
      "Epoch 1, loss: 2.302984\n",
      "Epoch 2, loss: 2.299926\n",
      "Epoch 3, loss: 2.299952\n",
      "Epoch 4, loss: 2.301453\n",
      "Epoch 5, loss: 2.301206\n",
      "Epoch 6, loss: 2.299648\n",
      "Epoch 7, loss: 2.299240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "15it [00:19,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 2.299108\n",
      "Epoch 9, loss: 2.300293\n",
      "lr=0.0001; reg=0.001\n",
      "Accuracy current:  0.133 \n",
      "\n",
      "Epoch 0, loss: 2.303414\n",
      "Epoch 1, loss: 2.302956\n",
      "Epoch 2, loss: 2.299897\n",
      "Epoch 3, loss: 2.299924\n",
      "Epoch 4, loss: 2.301425\n",
      "Epoch 5, loss: 2.301178\n",
      "Epoch 6, loss: 2.299620\n",
      "Epoch 7, loss: 2.299211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "16it [00:20,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 2.299080\n",
      "Epoch 9, loss: 2.300265\n",
      "lr=0.0001; reg=0.0001\n",
      "Accuracy current:  0.133 \n",
      "\n",
      "best validation accuracy achieved: 0.243000\n",
      "CPU times: user 2min 14s, sys: 1min 46s, total: 4min 1s\n",
      "Wall time: 20.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "reg_strengths = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "for lr, reg in tqdm(product(learning_rates, reg_strengths)):\n",
    "    classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "    classifier.fit(train_X, train_y, epochs=10, learning_rate=lr, \n",
    "                   batch_size=100, reg=reg, random_state=77)\n",
    "    \n",
    "    # make prediction on validation\n",
    "    pred = classifier.predict(val_X)\n",
    "    accuracy = multiclass_accuracy(pred, val_y)\n",
    "    print('lr={lr}; reg={reg}'.format(lr=lr, reg=reg))\n",
    "    print(\"Accuracy current: \", accuracy, '\\n')\n",
    "    \n",
    "    if (best_val_accuracy is None) or (best_val_accuracy < accuracy):\n",
    "        best_val_accuracy = accuracy\n",
    "        best_classifier = classifier\n",
    "    \n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:58:53.409879Z",
     "start_time": "2019-06-12T19:58:53.395227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.204000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
