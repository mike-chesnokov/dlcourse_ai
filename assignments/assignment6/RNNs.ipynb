{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
    "\n",
    "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P59NYU98GCb9"
   },
   "outputs": [],
   "source": [
    "!pip3 -qq install torch==0.4.1\n",
    "!pip3 -qq install bokeh==0.13.0\n",
    "!pip3 -qq install gensim==3.6.0\n",
    "!pip3 -qq install nltk\n",
    "!pip3 -qq install scikit-learn==0.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:44:33.083940Z",
     "start_time": "2019-09-07T15:44:32.798494Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8sVtGHmA9aBM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6CNKM3b4hT1"
   },
   "source": [
    "# Рекуррентные нейронные сети (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_XkoGNQUeGm"
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFEtWrS_4rUs"
   },
   "source": [
    "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
    "\n",
    "Мы порешаем сейчас POS Tagging для английского.\n",
    "\n",
    "Будем работать с таким набором тегов:\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition (on, of, at, ...)\n",
    "- ADV - adverb (really, already, still, ...)\n",
    "- CONJ - conjunction (and, or, but, ...)\n",
    "- DET - determiner, article (the, a, some, ...)\n",
    "- NOUN - noun (year, home, costs, ...)\n",
    "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
    "- PRT - particle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- . - punctuation marks (. , ;)\n",
    "- X - other (ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPIkKdFlHB-X"
   },
   "source": [
    "Скачаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T11:14:09.554112Z",
     "start_time": "2019-09-01T11:14:06.391085Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/mikhail/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/mikhail/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:44:38.547420Z",
     "start_time": "2019-09-07T15:44:38.198101Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "TiA2dGmgF1rW"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d93g_swyJA_V"
   },
   "source": [
    "Пример размеченного предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:44:39.607524Z",
     "start_time": "2019-09-07T15:44:39.595232Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "QstS4NO0L97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The            \tDET\n",
      "Fulton         \tNOUN\n",
      "County         \tNOUN\n",
      "Grand          \tADJ\n",
      "Jury           \tNOUN\n",
      "said           \tVERB\n",
      "Friday         \tNOUN\n",
      "an             \tDET\n",
      "investigation  \tNOUN\n",
      "of             \tADP\n",
      "Atlanta's      \tNOUN\n",
      "recent         \tADJ\n",
      "primary        \tNOUN\n",
      "election       \tNOUN\n",
      "produced       \tVERB\n",
      "``             \t.\n",
      "no             \tDET\n",
      "evidence       \tNOUN\n",
      "''             \t.\n",
      "that           \tADP\n",
      "any            \tDET\n",
      "irregularities \tNOUN\n",
      "took           \tVERB\n",
      "place          \tNOUN\n",
      ".              \t.\n"
     ]
    }
   ],
   "source": [
    "for word, tag in data[0]:\n",
    "    print('{:15}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epdW8u_YXcAv"
   },
   "source": [
    "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
    "\n",
    "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:45:01.338925Z",
     "start_time": "2019-09-07T15:44:46.709450Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "xTai8Ta0lgwL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count in train set: 739769\n",
      "Words count in val set: 130954\n",
      "Words count in test set: 290469\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
    "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
    "print('Words count in test set:', sum(len(sent) for sent in test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:45:03.582603Z",
     "start_time": "2019-09-07T15:45:03.566712Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('``', '.'),\n",
       " ('What', 'DET'),\n",
       " ('did', 'VERB'),\n",
       " ('you', 'PRON'),\n",
       " ('think', 'VERB'),\n",
       " ('about', 'ADP'),\n",
       " (\"Bang-Jensen's\", 'NOUN'),\n",
       " ('contention', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('errors', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('omissions', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('Hungarian', 'ADJ'),\n",
       " ('report', 'NOUN'),\n",
       " (\"''\", '.'),\n",
       " ('?', '.'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eChdLNGtXyP0"
   },
   "source": [
    "Построим маппинги из слов в индекс и из тега в индекс:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:45:05.991380Z",
     "start_time": "2019-09-07T15:45:05.766886Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "pCjwwDs6Zq9x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in train = 45441. Tags = {'NUM', 'DET', 'ADV', 'ADJ', 'NOUN', 'PRON', '.', 'CONJ', 'X', 'ADP', 'VERB', 'PRT'}\n"
     ]
    }
   ],
   "source": [
    "words = {word for sample in train_data for word, tag in sample}\n",
    "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
    "word2ind['<pad>'] = 0\n",
    "\n",
    "tags = {tag for sample in train_data for word, tag in sample}\n",
    "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
    "tag2ind['<pad>'] = 0\n",
    "\n",
    "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:45:12.867161Z",
     "start_time": "2019-09-07T15:45:12.388009Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "URC1B2nvPGFt"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
    "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(tags)), tags)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gArQwbzWWkgi"
   },
   "source": [
    "## Бейзлайн\n",
    "\n",
    "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
    "\n",
    "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
    "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
    "\n",
    "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
    "\n",
    "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
    "\n",
    "Простейший вариант - униграммная модель, учитывающая только слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T11:21:09.132900Z",
     "start_time": "2019-09-01T11:21:07.487612Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5rWmSToIaeAo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unigram tagger = 92.62%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
    "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Ymb_MkbWsF"
   },
   "source": [
    "Добавим вероятности переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T11:21:25.493947Z",
     "start_time": "2019-09-01T11:21:23.113113Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "vjz_Rk0bbMyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram tagger = 93.42%\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWMw6QHvbaDd"
   },
   "source": [
    "Обратите внимание, что `backoff` важен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T11:22:08.255449Z",
     "start_time": "2019-09-01T11:22:05.763996Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8XCuxEBVbOY_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger = 23.33%\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(train_data)\n",
    "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t3xyYd__8d-"
   },
   "source": [
    "## Увеличиваем контекст с рекуррентными сетями\n",
    "\n",
    "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
    "\n",
    "Омонимия - основная причина, почему униграмная модель плоха:  \n",
    "*“he cashed a check at the **bank**”*  \n",
    "vs  \n",
    "*“he sat on the **bank** of the river”*\n",
    "\n",
    "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
    "\n",
    "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
    "\n",
    "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
    "\n",
    "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:45:20.294151Z",
     "start_time": "2019-09-07T15:45:19.703726Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "RtRbz1SwgEqc"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, word2ind, tag2ind):\n",
    "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
    "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:45:23.395610Z",
     "start_time": "2019-09-07T15:45:23.391713Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "DhsTKZalfih6"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    X, y = data\n",
    "    n_samples = len(X)\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_indices = indices[start:end]\n",
    "        \n",
    "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
    "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        \n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "            \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:45:24.391232Z",
     "start_time": "2019-09-07T15:45:24.387665Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "l4XsRII5kW5x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 4), (32, 4))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
    "\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5I9E9P6eFYv"
   },
   "source": [
    "**Задание** Реализуйте `LSTMTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:45:27.017358Z",
     "start_time": "2019-09-07T15:45:27.013332Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "WVEHju54d68T"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        # embedding layer\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        \n",
    "        # lstm layer\n",
    "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim)\n",
    "        \n",
    "        # linear layer outputs tag\n",
    "        self.hidden2tag = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # make embeddings from input\n",
    "        embeds = self.word_embeddings(inputs)\n",
    "        # pass through lstm layer\n",
    "        lstm_out, _ = self.lstm(embeds.view(inputs.numel(), 1, -1))\n",
    "        # pass through linear layer\n",
    "        tag_space = self.hidden2tag(lstm_out.view(inputs.numel(), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        \n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_HA8zyheYGH"
   },
   "source": [
    "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:45:32.757079Z",
     "start_time": "2019-09-07T15:45:32.674875Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "jbrxsZ2mehWB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 128)\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "logits = model(X_batch)\n",
    "\n",
    "def calc_accuracy(target, preds):\n",
    "    \"\"\"\n",
    "    Calculate accuracy for model\n",
    "    \"\"\"\n",
    "    _, inds = torch.max(preds, dim=1)\n",
    "    correct_samples = torch.sum(target.view(-1) == inds).item()\n",
    "    \n",
    "    return correct_samples, inds.numel()\n",
    "\n",
    "print(calc_accuracy(y_batch, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T13:58:37.657900Z",
     "start_time": "2019-09-01T13:58:37.644923Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "GMUyUm1hgpe3",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.589792251586914"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "loss_value = criterion(logits, y_batch.view(-1)).item()\n",
    "loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSgV3NPUpcjH"
   },
   "source": [
    "**Задание** Вставьте эти вычисление в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:45:37.526035Z",
     "start_time": "2019-09-07T15:45:37.516117Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "FprPQ0gllo7b"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                \n",
    "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "                logits = model(X_batch)\n",
    "\n",
    "                loss = criterion(logits, y_batch.view(-1))\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                #cur_correct_count, cur_sum_count = calc_accuracy(y_batch, logits)\n",
    "                cur_correct_count, cur_sum_count = calc_accuracy_pad(y_batch, logits)\n",
    "                \n",
    "                correct_count += cur_correct_count\n",
    "                sum_count += cur_sum_count\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
    "                )\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count, correct_count / sum_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, \n",
    "        epochs_count=1, batch_size=32,\n",
    "        val_data=None, val_batch_size=None):\n",
    "        \n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, \n",
    "                                         optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_data is None:\n",
    "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, \n",
    "                                         None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T14:49:54.764663Z",
     "start_time": "2019-09-01T13:58:41.583344Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Pqfbeh1ltEYa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.32777, Accuracy = 90.32%: 100%|██████████| 572/572 [00:54<00:00, 10.58it/s]\n",
      "[1 / 50]   Val: Loss = 0.13688, Accuracy = 96.93%: 100%|██████████| 13/13 [00:07<00:00,  1.78it/s]\n",
      "[2 / 50] Train: Loss = 0.10981, Accuracy = 96.51%: 100%|██████████| 572/572 [00:54<00:00, 10.47it/s]\n",
      "[2 / 50]   Val: Loss = 0.11584, Accuracy = 97.76%: 100%|██████████| 13/13 [00:07<00:00,  1.79it/s]\n",
      "[3 / 50] Train: Loss = 0.07967, Accuracy = 97.41%: 100%|██████████| 572/572 [00:54<00:00, 10.56it/s]\n",
      "[3 / 50]   Val: Loss = 0.10839, Accuracy = 98.11%: 100%|██████████| 13/13 [00:07<00:00,  1.82it/s]\n",
      "[4 / 50] Train: Loss = 0.06490, Accuracy = 97.81%: 100%|██████████| 572/572 [00:54<00:00, 10.59it/s]\n",
      "[4 / 50]   Val: Loss = 0.09859, Accuracy = 98.36%: 100%|██████████| 13/13 [00:07<00:00,  1.71it/s]\n",
      "[5 / 50] Train: Loss = 0.05622, Accuracy = 98.05%: 100%|██████████| 572/572 [00:54<00:00, 10.57it/s]\n",
      "[5 / 50]   Val: Loss = 0.10646, Accuracy = 98.34%: 100%|██████████| 13/13 [00:07<00:00,  1.83it/s]\n",
      "[6 / 50] Train: Loss = 0.05058, Accuracy = 98.20%: 100%|██████████| 572/572 [00:54<00:00, 10.58it/s]\n",
      "[6 / 50]   Val: Loss = 0.10215, Accuracy = 98.44%: 100%|██████████| 13/13 [00:07<00:00,  1.79it/s]\n",
      "[7 / 50] Train: Loss = 0.04611, Accuracy = 98.31%: 100%|██████████| 572/572 [00:54<00:00, 10.57it/s]\n",
      "[7 / 50]   Val: Loss = 0.09957, Accuracy = 98.50%: 100%|██████████| 13/13 [00:07<00:00,  1.73it/s]\n",
      "[8 / 50] Train: Loss = 0.04325, Accuracy = 98.39%: 100%|██████████| 572/572 [00:54<00:00, 10.55it/s]\n",
      "[8 / 50]   Val: Loss = 0.10023, Accuracy = 98.55%: 100%|██████████| 13/13 [00:07<00:00,  1.65it/s]\n",
      "[9 / 50] Train: Loss = 0.04055, Accuracy = 98.46%: 100%|██████████| 572/572 [00:53<00:00, 10.60it/s]\n",
      "[9 / 50]   Val: Loss = 0.11881, Accuracy = 98.39%: 100%|██████████| 13/13 [00:06<00:00,  1.93it/s]\n",
      "[10 / 50] Train: Loss = 0.03886, Accuracy = 98.50%: 100%|██████████| 572/572 [00:54<00:00, 10.50it/s]\n",
      "[10 / 50]   Val: Loss = 0.12254, Accuracy = 98.36%: 100%|██████████| 13/13 [00:06<00:00,  1.91it/s]\n",
      "[11 / 50] Train: Loss = 0.03747, Accuracy = 98.53%: 100%|██████████| 572/572 [00:54<00:00, 10.52it/s]\n",
      "[11 / 50]   Val: Loss = 0.11414, Accuracy = 98.47%: 100%|██████████| 13/13 [00:07<00:00,  1.84it/s]\n",
      "[12 / 50] Train: Loss = 0.03651, Accuracy = 98.55%: 100%|██████████| 572/572 [00:54<00:00, 10.56it/s]\n",
      "[12 / 50]   Val: Loss = 0.11083, Accuracy = 98.50%: 100%|██████████| 13/13 [00:07<00:00,  1.81it/s]\n",
      "[13 / 50] Train: Loss = 0.03535, Accuracy = 98.58%: 100%|██████████| 572/572 [00:54<00:00, 10.54it/s]\n",
      "[13 / 50]   Val: Loss = 0.12026, Accuracy = 98.40%: 100%|██████████| 13/13 [00:06<00:00,  1.91it/s]\n",
      "[14 / 50] Train: Loss = 0.03465, Accuracy = 98.60%: 100%|██████████| 572/572 [00:54<00:00, 10.52it/s]\n",
      "[14 / 50]   Val: Loss = 0.11428, Accuracy = 98.54%: 100%|██████████| 13/13 [00:07<00:00,  1.80it/s]\n",
      "[15 / 50] Train: Loss = 0.03391, Accuracy = 98.62%: 100%|██████████| 572/572 [00:54<00:00, 10.49it/s]\n",
      "[15 / 50]   Val: Loss = 0.11484, Accuracy = 98.49%: 100%|██████████| 13/13 [00:07<00:00,  1.79it/s]\n",
      "[16 / 50] Train: Loss = 0.03400, Accuracy = 98.61%: 100%|██████████| 572/572 [00:54<00:00, 10.56it/s]\n",
      "[16 / 50]   Val: Loss = 0.11477, Accuracy = 98.55%: 100%|██████████| 13/13 [00:07<00:00,  1.77it/s]\n",
      "[17 / 50] Train: Loss = 0.03325, Accuracy = 98.63%: 100%|██████████| 572/572 [00:54<00:00, 10.46it/s]\n",
      "[17 / 50]   Val: Loss = 0.11200, Accuracy = 98.52%: 100%|██████████| 13/13 [00:07<00:00,  1.79it/s]\n",
      "[18 / 50] Train: Loss = 0.03328, Accuracy = 98.62%: 100%|██████████| 572/572 [00:54<00:00, 10.56it/s]\n",
      "[18 / 50]   Val: Loss = 0.12499, Accuracy = 98.44%: 100%|██████████| 13/13 [00:06<00:00,  1.89it/s]\n",
      "[19 / 50] Train: Loss = 0.03274, Accuracy = 98.64%: 100%|██████████| 572/572 [00:53<00:00, 10.63it/s]\n",
      "[19 / 50]   Val: Loss = 0.11778, Accuracy = 98.56%: 100%|██████████| 13/13 [00:07<00:00,  1.68it/s]\n",
      "[20 / 50] Train: Loss = 0.03264, Accuracy = 98.64%: 100%|██████████| 572/572 [00:54<00:00, 10.45it/s]\n",
      "[20 / 50]   Val: Loss = 0.12078, Accuracy = 98.48%: 100%|██████████| 13/13 [00:07<00:00,  1.75it/s]\n",
      "[21 / 50] Train: Loss = 0.03198, Accuracy = 98.66%: 100%|██████████| 572/572 [00:54<00:00, 10.49it/s]\n",
      "[21 / 50]   Val: Loss = 0.12829, Accuracy = 98.38%: 100%|██████████| 13/13 [00:06<00:00,  1.94it/s]\n",
      "[22 / 50] Train: Loss = 0.03198, Accuracy = 98.66%: 100%|██████████| 572/572 [00:54<00:00, 10.54it/s]\n",
      "[22 / 50]   Val: Loss = 0.11647, Accuracy = 98.52%: 100%|██████████| 13/13 [00:07<00:00,  1.75it/s]\n",
      "[23 / 50] Train: Loss = 0.03184, Accuracy = 98.66%: 100%|██████████| 572/572 [00:55<00:00, 10.39it/s]\n",
      "[23 / 50]   Val: Loss = 0.11846, Accuracy = 98.51%: 100%|██████████| 13/13 [00:07<00:00,  1.78it/s]\n",
      "[24 / 50] Train: Loss = 0.03182, Accuracy = 98.66%: 100%|██████████| 572/572 [00:54<00:00, 10.45it/s]\n",
      "[24 / 50]   Val: Loss = 0.11553, Accuracy = 98.55%: 100%|██████████| 13/13 [00:07<00:00,  1.72it/s]\n",
      "[25 / 50] Train: Loss = 0.03155, Accuracy = 98.67%: 100%|██████████| 572/572 [00:54<00:00, 10.41it/s]\n",
      "[25 / 50]   Val: Loss = 0.11927, Accuracy = 98.54%: 100%|██████████| 13/13 [00:07<00:00,  1.78it/s]\n",
      "[26 / 50] Train: Loss = 0.03133, Accuracy = 98.68%: 100%|██████████| 572/572 [00:53<00:00, 10.61it/s]\n",
      "[26 / 50]   Val: Loss = 0.11938, Accuracy = 98.55%: 100%|██████████| 13/13 [00:07<00:00,  1.75it/s]\n",
      "[27 / 50] Train: Loss = 0.03144, Accuracy = 98.67%: 100%|██████████| 572/572 [00:54<00:00, 10.49it/s]\n",
      "[27 / 50]   Val: Loss = 0.12559, Accuracy = 98.51%: 100%|██████████| 13/13 [00:07<00:00,  1.82it/s]\n",
      "[28 / 50] Train: Loss = 0.03133, Accuracy = 98.68%: 100%|██████████| 572/572 [00:54<00:00, 10.46it/s]\n",
      "[28 / 50]   Val: Loss = 0.12380, Accuracy = 98.52%: 100%|██████████| 13/13 [00:07<00:00,  1.80it/s]\n",
      "[29 / 50] Train: Loss = 0.03109, Accuracy = 98.69%: 100%|██████████| 572/572 [00:54<00:00, 10.49it/s]\n",
      "[29 / 50]   Val: Loss = 0.11566, Accuracy = 98.55%: 100%|██████████| 13/13 [00:07<00:00,  1.73it/s]\n",
      "[30 / 50] Train: Loss = 0.03126, Accuracy = 98.68%: 100%|██████████| 572/572 [00:54<00:00, 10.47it/s]\n",
      "[30 / 50]   Val: Loss = 0.12575, Accuracy = 98.48%: 100%|██████████| 13/13 [00:07<00:00,  1.84it/s]\n",
      "[31 / 50] Train: Loss = 0.03109, Accuracy = 98.68%: 100%|██████████| 572/572 [00:54<00:00, 10.48it/s]\n",
      "[31 / 50]   Val: Loss = 0.11444, Accuracy = 98.60%: 100%|██████████| 13/13 [00:07<00:00,  1.74it/s]\n",
      "[32 / 50] Train: Loss = 0.03096, Accuracy = 98.68%: 100%|██████████| 572/572 [00:54<00:00, 10.55it/s]\n",
      "[32 / 50]   Val: Loss = 0.13409, Accuracy = 98.41%: 100%|██████████| 13/13 [00:06<00:00,  1.91it/s]\n",
      "[33 / 50] Train: Loss = 0.03127, Accuracy = 98.68%: 100%|██████████| 572/572 [00:53<00:00, 10.64it/s]\n",
      "[33 / 50]   Val: Loss = 0.12843, Accuracy = 98.53%: 100%|██████████| 13/13 [00:07<00:00,  1.83it/s]\n",
      "[34 / 50] Train: Loss = 0.03085, Accuracy = 98.69%: 100%|██████████| 572/572 [00:54<00:00, 10.56it/s]\n",
      "[34 / 50]   Val: Loss = 0.13350, Accuracy = 98.53%: 100%|██████████| 13/13 [00:07<00:00,  1.82it/s]\n",
      "[35 / 50] Train: Loss = 0.03079, Accuracy = 98.69%: 100%|██████████| 572/572 [00:54<00:00, 10.56it/s]\n",
      "[35 / 50]   Val: Loss = 0.14328, Accuracy = 98.41%: 100%|██████████| 13/13 [00:06<00:00,  1.87it/s]\n",
      "[36 / 50] Train: Loss = 0.03073, Accuracy = 98.69%: 100%|██████████| 572/572 [00:54<00:00, 10.52it/s]\n",
      "[36 / 50]   Val: Loss = 0.13245, Accuracy = 98.53%: 100%|██████████| 13/13 [00:07<00:00,  1.84it/s]\n",
      "[37 / 50] Train: Loss = 0.03052, Accuracy = 98.70%: 100%|██████████| 572/572 [00:54<00:00, 10.51it/s]\n",
      "[37 / 50]   Val: Loss = 0.13238, Accuracy = 98.52%: 100%|██████████| 13/13 [00:07<00:00,  1.81it/s]\n",
      "[38 / 50] Train: Loss = 0.03082, Accuracy = 98.69%: 100%|██████████| 572/572 [00:53<00:00, 10.60it/s]\n",
      "[38 / 50]   Val: Loss = 0.14148, Accuracy = 98.56%: 100%|██████████| 13/13 [00:07<00:00,  1.78it/s]\n",
      "[39 / 50] Train: Loss = 0.03043, Accuracy = 98.70%: 100%|██████████| 572/572 [00:54<00:00, 10.50it/s]\n",
      "[39 / 50]   Val: Loss = 0.12589, Accuracy = 98.54%: 100%|██████████| 13/13 [00:07<00:00,  1.70it/s]\n",
      "[40 / 50] Train: Loss = 0.03039, Accuracy = 98.70%: 100%|██████████| 572/572 [00:54<00:00, 10.57it/s]\n",
      "[40 / 50]   Val: Loss = 0.14127, Accuracy = 98.49%: 100%|██████████| 13/13 [00:06<00:00,  1.86it/s]\n",
      "[41 / 50] Train: Loss = 0.03068, Accuracy = 98.69%: 100%|██████████| 572/572 [00:54<00:00, 10.58it/s]\n",
      "[41 / 50]   Val: Loss = 0.12194, Accuracy = 98.60%: 100%|██████████| 13/13 [00:07<00:00,  1.78it/s]\n",
      "[42 / 50] Train: Loss = 0.03041, Accuracy = 98.70%: 100%|██████████| 572/572 [00:52<00:00, 10.97it/s]\n",
      "[42 / 50]   Val: Loss = 0.12959, Accuracy = 98.60%: 100%|██████████| 13/13 [00:07<00:00,  1.80it/s]\n",
      "[43 / 50] Train: Loss = 0.03062, Accuracy = 98.70%: 100%|██████████| 572/572 [00:51<00:00, 11.00it/s]\n",
      "[43 / 50]   Val: Loss = 0.14247, Accuracy = 98.45%: 100%|██████████| 13/13 [00:06<00:00,  1.99it/s]\n",
      "[44 / 50] Train: Loss = 0.03074, Accuracy = 98.70%: 100%|██████████| 572/572 [00:52<00:00, 10.99it/s]\n",
      "[44 / 50]   Val: Loss = 0.14769, Accuracy = 98.48%: 100%|██████████| 13/13 [00:06<00:00,  1.94it/s]\n",
      "[45 / 50] Train: Loss = 0.03020, Accuracy = 98.71%: 100%|██████████| 572/572 [00:54<00:00, 10.45it/s]\n",
      "[45 / 50]   Val: Loss = 0.13443, Accuracy = 98.61%: 100%|██████████| 13/13 [00:07<00:00,  1.65it/s]\n",
      "[46 / 50] Train: Loss = 0.03017, Accuracy = 98.71%: 100%|██████████| 572/572 [00:55<00:00, 10.39it/s]\n",
      "[46 / 50]   Val: Loss = 0.14852, Accuracy = 98.52%: 100%|██████████| 13/13 [00:07<00:00,  1.77it/s]\n",
      "[47 / 50] Train: Loss = 0.03013, Accuracy = 98.71%: 100%|██████████| 572/572 [00:55<00:00, 10.37it/s]\n",
      "[47 / 50]   Val: Loss = 0.14441, Accuracy = 98.48%: 100%|██████████| 13/13 [00:07<00:00,  1.82it/s]\n",
      "[48 / 50] Train: Loss = 0.03017, Accuracy = 98.72%: 100%|██████████| 572/572 [00:54<00:00, 10.44it/s]\n",
      "[48 / 50]   Val: Loss = 0.13593, Accuracy = 98.57%: 100%|██████████| 13/13 [00:07<00:00,  1.74it/s]\n",
      "[49 / 50] Train: Loss = 0.03007, Accuracy = 98.72%: 100%|██████████| 572/572 [00:54<00:00, 10.44it/s]\n",
      "[49 / 50]   Val: Loss = 0.15194, Accuracy = 98.36%: 100%|██████████| 13/13 [00:06<00:00,  1.93it/s]\n",
      "[50 / 50] Train: Loss = 0.03021, Accuracy = 98.71%: 100%|██████████| 572/572 [00:54<00:00, 10.49it/s]\n",
      "[50 / 50]   Val: Loss = 0.15428, Accuracy = 98.48%: 100%|██████████| 13/13 [00:07<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45min 11s, sys: 6min 46s, total: 51min 57s\n",
      "Wall time: 51min 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), \n",
    "    epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), \n",
    "    val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0qGetIhfUE5"
   },
   "source": [
    "### Masking\n",
    "\n",
    "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
    "\n",
    "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T15:08:07.408601Z",
     "start_time": "2019-09-01T15:08:07.360866Z"
    }
   },
   "outputs": [],
   "source": [
    "model_2 = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "logits = model_2(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:45:45.447931Z",
     "start_time": "2019-09-07T15:45:45.425602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 128)\n",
      "(7, 92)\n"
     ]
    }
   ],
   "source": [
    "def calc_accuracy_pad(target, preds):\n",
    "    \"\"\"\n",
    "    Calculate accuracy for model\n",
    "    \"\"\"\n",
    "    zero_mask = (target == 0).view(-1)\n",
    "    _, inds = torch.max(preds[1-zero_mask], dim=1)\n",
    "    correct_samples = torch.sum(target.view(-1)[1-zero_mask] == inds).item()\n",
    "    \n",
    "    return correct_samples, inds.numel()\n",
    "\n",
    "print(calc_accuracy(y_batch, logits))\n",
    "print(calc_accuracy_pad(y_batch, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T15:09:04.364256Z",
     "start_time": "2019-09-01T15:09:04.360254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5266876220703125 2.55135440826416\n"
     ]
    }
   ],
   "source": [
    "criterion1 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.CrossEntropyLoss(ignore_index=0)\n",
    "loss_value1 = criterion1(logits, y_batch.view(-1)).item()\n",
    "loss_value2 = criterion2(logits, y_batch.view(-1)).item()\n",
    "print(loss_value1, loss_value2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAfV2dEOfHo5"
   },
   "source": [
    "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:45:53.674123Z",
     "start_time": "2019-09-07T15:45:53.668706Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "98wr38_rw55D"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data, batch_size):\n",
    "    \n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    model.eval() # evaluation mode\n",
    "    \n",
    "    for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                \n",
    "        X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "        logits = model(X_batch)\n",
    "\n",
    "        cur_correct_count, cur_sum_count = calc_accuracy_pad(y_batch, logits)\n",
    "        #cur_correct_count, cur_sum_count = calc_accuracy(y_batch, logits)\n",
    "        \n",
    "        correct_count += cur_correct_count\n",
    "        sum_count += cur_sum_count\n",
    "    \n",
    "    return float(correct_count/sum_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T19:08:55.214036Z",
     "start_time": "2019-09-01T19:08:46.282968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9726758612730105\n",
      "CPU times: user 7.66 s, sys: 1.28 s, total: 8.94 s\n",
      "Wall time: 8.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# calc_accuracy\n",
    "res = evaluate_model(model, (X_test, y_test), batch_size=32)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T18:50:39.497517Z",
     "start_time": "2019-09-01T18:50:30.385113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9251486389253242\n",
      "CPU times: user 7.98 s, sys: 1.14 s, total: 9.12 s\n",
      "Wall time: 9.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# calc_accuracy_pad\n",
    "res = evaluate_model(model, (X_test, y_test), batch_size=32)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXUTSFaEHbDG"
   },
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
    "\n",
    "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
    "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
    "\n",
    "**Задание** Добавьте Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:45:58.998686Z",
     "start_time": "2019-09-07T15:45:58.991028Z"
    }
   },
   "outputs": [],
   "source": [
    "class BidirLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        # embedding layer\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        \n",
    "        # lstm layer\n",
    "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, bidirectional=True)\n",
    "        \n",
    "        # linear layer outputs tag\n",
    "        self.hidden2tag = nn.Linear(lstm_hidden_dim*2, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # make embeddings from input\n",
    "        embeds = self.word_embeddings(inputs)\n",
    "        # pass through lstm layer\n",
    "        lstm_out, _ = self.lstm(embeds.view(inputs.numel(), 1, -1))\n",
    "        # pass through linear layer\n",
    "        tag_space = self.hidden2tag(lstm_out.view(inputs.numel(), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        \n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T20:11:00.374140Z",
     "start_time": "2019-09-01T19:26:26.760444Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 20] Train: Loss = 0.50748, Accuracy = 43.77%: 100%|██████████| 143/143 [02:04<00:00,  1.15it/s]\n",
      "[1 / 20]   Val: Loss = 0.23760, Accuracy = 66.08%: 100%|██████████| 13/13 [00:14<00:00,  1.12s/it]\n",
      "[2 / 20] Train: Loss = 0.18640, Accuracy = 76.54%: 100%|██████████| 143/143 [02:01<00:00,  1.18it/s]\n",
      "[2 / 20]   Val: Loss = 0.14490, Accuracy = 81.55%: 100%|██████████| 13/13 [00:14<00:00,  1.12s/it]\n",
      "[3 / 20] Train: Loss = 0.10744, Accuracy = 86.34%: 100%|██████████| 143/143 [02:01<00:00,  1.18it/s]\n",
      "[3 / 20]   Val: Loss = 0.11628, Accuracy = 86.79%: 100%|██████████| 13/13 [00:14<00:00,  1.08s/it]\n",
      "[4 / 20] Train: Loss = 0.07845, Accuracy = 90.01%: 100%|██████████| 143/143 [01:59<00:00,  1.19it/s]\n",
      "[4 / 20]   Val: Loss = 0.10893, Accuracy = 88.92%: 100%|██████████| 13/13 [00:13<00:00,  1.05s/it]\n",
      "[5 / 20] Train: Loss = 0.06412, Accuracy = 91.62%: 100%|██████████| 143/143 [01:59<00:00,  1.19it/s]\n",
      "[5 / 20]   Val: Loss = 0.10810, Accuracy = 89.94%: 100%|██████████| 13/13 [00:13<00:00,  1.05s/it]\n",
      "[6 / 20] Train: Loss = 0.05641, Accuracy = 92.57%: 100%|██████████| 143/143 [01:58<00:00,  1.21it/s]\n",
      "[6 / 20]   Val: Loss = 0.10152, Accuracy = 90.57%: 100%|██████████| 13/13 [00:14<00:00,  1.08s/it]\n",
      "[7 / 20] Train: Loss = 0.05071, Accuracy = 93.23%: 100%|██████████| 143/143 [01:58<00:00,  1.21it/s]\n",
      "[7 / 20]   Val: Loss = 0.10672, Accuracy = 91.02%: 100%|██████████| 13/13 [00:13<00:00,  1.02s/it]\n",
      "[8 / 20] Train: Loss = 0.04533, Accuracy = 93.70%: 100%|██████████| 143/143 [01:59<00:00,  1.19it/s]\n",
      "[8 / 20]   Val: Loss = 0.10425, Accuracy = 91.33%: 100%|██████████| 13/13 [00:13<00:00,  1.04s/it]\n",
      "[9 / 20] Train: Loss = 0.04224, Accuracy = 94.07%: 100%|██████████| 143/143 [01:58<00:00,  1.20it/s]\n",
      "[9 / 20]   Val: Loss = 0.10285, Accuracy = 91.63%: 100%|██████████| 13/13 [00:13<00:00,  1.06s/it]\n",
      "[10 / 20] Train: Loss = 0.03981, Accuracy = 94.33%: 100%|██████████| 143/143 [01:58<00:00,  1.21it/s]\n",
      "[10 / 20]   Val: Loss = 0.10491, Accuracy = 91.78%: 100%|██████████| 13/13 [00:13<00:00,  1.01s/it]\n",
      "[11 / 20] Train: Loss = 0.03775, Accuracy = 94.57%: 100%|██████████| 143/143 [01:57<00:00,  1.21it/s]\n",
      "[11 / 20]   Val: Loss = 0.10240, Accuracy = 91.84%: 100%|██████████| 13/13 [00:13<00:00,  1.05s/it]\n",
      "[12 / 20] Train: Loss = 0.03584, Accuracy = 94.75%: 100%|██████████| 143/143 [01:58<00:00,  1.20it/s]\n",
      "[12 / 20]   Val: Loss = 0.09963, Accuracy = 92.00%: 100%|██████████| 13/13 [00:14<00:00,  1.13s/it]\n",
      "[13 / 20] Train: Loss = 0.03402, Accuracy = 94.90%: 100%|██████████| 143/143 [02:04<00:00,  1.15it/s]\n",
      "[13 / 20]   Val: Loss = 0.10057, Accuracy = 92.13%: 100%|██████████| 13/13 [00:14<00:00,  1.09s/it]\n",
      "[14 / 20] Train: Loss = 0.03282, Accuracy = 95.02%: 100%|██████████| 143/143 [02:02<00:00,  1.17it/s]\n",
      "[14 / 20]   Val: Loss = 0.10284, Accuracy = 92.08%: 100%|██████████| 13/13 [00:14<00:00,  1.08s/it]\n",
      "[15 / 20] Train: Loss = 0.03205, Accuracy = 95.13%: 100%|██████████| 143/143 [02:00<00:00,  1.19it/s]\n",
      "[15 / 20]   Val: Loss = 0.10281, Accuracy = 92.26%: 100%|██████████| 13/13 [00:13<00:00,  1.04s/it]\n",
      "[16 / 20] Train: Loss = 0.03070, Accuracy = 95.21%: 100%|██████████| 143/143 [01:58<00:00,  1.21it/s]\n",
      "[16 / 20]   Val: Loss = 0.09955, Accuracy = 92.29%: 100%|██████████| 13/13 [00:13<00:00,  1.04s/it]\n",
      "[17 / 20] Train: Loss = 0.03006, Accuracy = 95.30%: 100%|██████████| 143/143 [01:58<00:00,  1.21it/s]\n",
      "[17 / 20]   Val: Loss = 0.10035, Accuracy = 92.33%: 100%|██████████| 13/13 [00:13<00:00,  1.07s/it]\n",
      "[18 / 20] Train: Loss = 0.02943, Accuracy = 95.34%: 100%|██████████| 143/143 [01:57<00:00,  1.22it/s]\n",
      "[18 / 20]   Val: Loss = 0.10391, Accuracy = 92.36%: 100%|██████████| 13/13 [00:13<00:00,  1.06s/it]\n",
      "[19 / 20] Train: Loss = 0.02870, Accuracy = 95.41%: 100%|██████████| 143/143 [01:58<00:00,  1.20it/s]\n",
      "[19 / 20]   Val: Loss = 0.10228, Accuracy = 92.38%: 100%|██████████| 13/13 [00:13<00:00,  1.05s/it]\n",
      "[20 / 20] Train: Loss = 0.02801, Accuracy = 95.44%: 100%|██████████| 143/143 [01:58<00:00,  1.20it/s]\n",
      "[20 / 20]   Val: Loss = 0.10240, Accuracy = 92.39%: 100%|██████████| 13/13 [00:13<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38min 48s, sys: 6min 5s, total: 44min 54s\n",
      "Wall time: 44min 33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_3 = BidirLSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model_3.parameters())\n",
    "\n",
    "fit(model_3, criterion, optimizer, train_data=(X_train, y_train), \n",
    "    epochs_count=20,\n",
    "    batch_size=256, val_data=(X_val, y_val), \n",
    "    val_batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T20:39:07.307996Z",
     "start_time": "2019-09-01T20:38:44.999226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9791789216971482\n",
      "CPU times: user 19.3 s, sys: 2.97 s, total: 22.3 s\n",
      "Wall time: 22.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# calc_accuracy_pad\n",
    "res = evaluate_model(model_3, (X_test, y_test), batch_size=128)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTXmYGD_ANhm"
   },
   "source": [
    "### Предобученные эмбеддинги\n",
    "\n",
    "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
    "\n",
    "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:47:23.123451Z",
     "start_time": "2019-09-07T15:46:22.620591Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "uZpY_Q1xZ18h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYogOoKlgtcf"
   },
   "source": [
    "Построим подматрицу для слов из нашей тренировочной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:47:41.491403Z",
     "start_time": "2019-09-07T15:47:41.393517Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "VsCstxiO03oT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know 38736 out of 45441 word embeddings\n"
     ]
    }
   ],
   "source": [
    "known_count = 0\n",
    "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
    "for word, ind in word2ind.items():\n",
    "    word = word.lower()\n",
    "    if word in w2v_model.vocab:\n",
    "        embeddings[ind] = w2v_model.get_vector(word)\n",
    "        known_count += 1\n",
    "        \n",
    "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcG7i-R8hbY3"
   },
   "source": [
    "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:56:57.219944Z",
     "start_time": "2019-09-07T15:56:57.214520Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "LxaRBpQd0pat"
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        \n",
    "        # embedding layer\n",
    "        embeddings_tch = FloatTensor(embeddings)\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(embeddings_tch)\n",
    "        word_emb_dim = embeddings_tch.shape[1]\n",
    "        # lstm layer\n",
    "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim)\n",
    "        \n",
    "        # linear layer outputs tag\n",
    "        self.hidden2tag = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # make embeddings from input\n",
    "        embeds = self.word_embeddings(inputs)\n",
    "        # pass through lstm layer\n",
    "        lstm_out, _ = self.lstm(embeds.view(inputs.numel(), 1, -1))\n",
    "        # pass through linear layer\n",
    "        tag_space = self.hidden2tag(lstm_out.view(inputs.numel(), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        \n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:57:14.282277Z",
     "start_time": "2019-09-07T15:57:14.262551Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "EBtI6BDE-Fc7"
   },
   "outputs": [],
   "source": [
    "model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=embeddings,\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T16:18:43.614091Z",
     "start_time": "2019-09-07T15:58:13.763141Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 20] Train: Loss = 0.79439, Accuracy = 76.12%: 100%|██████████| 572/572 [00:52<00:00, 10.89it/s]\n",
      "[1 / 20]   Val: Loss = 0.41757, Accuracy = 87.53%: 100%|██████████| 13/13 [00:07<00:00,  1.84it/s]\n",
      "[2 / 20] Train: Loss = 0.33638, Accuracy = 89.77%: 100%|██████████| 572/572 [00:52<00:00, 10.86it/s]\n",
      "[2 / 20]   Val: Loss = 0.31302, Accuracy = 90.11%: 100%|██████████| 13/13 [00:07<00:00,  1.78it/s]\n",
      "[3 / 20] Train: Loss = 0.26761, Accuracy = 91.55%: 100%|██████████| 572/572 [00:52<00:00, 10.88it/s]\n",
      "[3 / 20]   Val: Loss = 0.26724, Accuracy = 91.40%: 100%|██████████| 13/13 [00:07<00:00,  1.79it/s]\n",
      "[4 / 20] Train: Loss = 0.23400, Accuracy = 92.36%: 100%|██████████| 572/572 [00:53<00:00, 10.77it/s]\n",
      "[4 / 20]   Val: Loss = 0.24423, Accuracy = 92.02%: 100%|██████████| 13/13 [00:07<00:00,  1.83it/s]\n",
      "[5 / 20] Train: Loss = 0.21435, Accuracy = 92.83%: 100%|██████████| 572/572 [00:52<00:00, 10.99it/s]\n",
      "[5 / 20]   Val: Loss = 0.22943, Accuracy = 92.42%: 100%|██████████| 13/13 [00:07<00:00,  1.86it/s]\n",
      "[6 / 20] Train: Loss = 0.20114, Accuracy = 93.25%: 100%|██████████| 572/572 [00:52<00:00, 10.84it/s]\n",
      "[6 / 20]   Val: Loss = 0.21851, Accuracy = 92.71%: 100%|██████████| 13/13 [00:07<00:00,  1.75it/s]\n",
      "[7 / 20] Train: Loss = 0.19192, Accuracy = 93.48%: 100%|██████████| 572/572 [00:52<00:00, 10.92it/s]\n",
      "[7 / 20]   Val: Loss = 0.21339, Accuracy = 92.87%: 100%|██████████| 13/13 [00:07<00:00,  1.71it/s]\n",
      "[8 / 20] Train: Loss = 0.18469, Accuracy = 93.66%: 100%|██████████| 572/572 [00:54<00:00, 10.43it/s]\n",
      "[8 / 20]   Val: Loss = 0.20675, Accuracy = 92.98%: 100%|██████████| 13/13 [00:07<00:00,  1.67it/s]\n",
      "[9 / 20] Train: Loss = 0.17905, Accuracy = 93.78%: 100%|██████████| 572/572 [00:55<00:00, 10.36it/s]\n",
      "[9 / 20]   Val: Loss = 0.20223, Accuracy = 93.12%: 100%|██████████| 13/13 [00:07<00:00,  1.78it/s]\n",
      "[10 / 20] Train: Loss = 0.17442, Accuracy = 93.87%: 100%|██████████| 572/572 [00:55<00:00, 10.38it/s]\n",
      "[10 / 20]   Val: Loss = 0.19859, Accuracy = 93.19%: 100%|██████████| 13/13 [00:06<00:00,  1.88it/s]\n",
      "[11 / 20] Train: Loss = 0.17074, Accuracy = 93.97%: 100%|██████████| 572/572 [00:55<00:00, 10.38it/s]\n",
      "[11 / 20]   Val: Loss = 0.19550, Accuracy = 93.20%: 100%|██████████| 13/13 [00:08<00:00,  1.62it/s]\n",
      "[12 / 20] Train: Loss = 0.16801, Accuracy = 94.01%: 100%|██████████| 572/572 [00:54<00:00, 10.52it/s]\n",
      "[12 / 20]   Val: Loss = 0.19340, Accuracy = 93.26%: 100%|██████████| 13/13 [00:07<00:00,  1.74it/s]\n",
      "[13 / 20] Train: Loss = 0.16544, Accuracy = 94.07%: 100%|██████████| 572/572 [00:54<00:00, 10.43it/s]\n",
      "[13 / 20]   Val: Loss = 0.19416, Accuracy = 93.28%: 100%|██████████| 13/13 [00:07<00:00,  1.63it/s]\n",
      "[14 / 20] Train: Loss = 0.16323, Accuracy = 94.09%: 100%|██████████| 572/572 [00:54<00:00, 10.43it/s]\n",
      "[14 / 20]   Val: Loss = 0.19021, Accuracy = 93.26%: 100%|██████████| 13/13 [00:08<00:00,  1.60it/s]\n",
      "[15 / 20] Train: Loss = 0.16134, Accuracy = 94.14%: 100%|██████████| 572/572 [00:54<00:00, 10.44it/s]\n",
      "[15 / 20]   Val: Loss = 0.18815, Accuracy = 93.33%: 100%|██████████| 13/13 [00:07<00:00,  1.71it/s]\n",
      "[16 / 20] Train: Loss = 0.15959, Accuracy = 94.19%: 100%|██████████| 572/572 [00:54<00:00, 10.44it/s]\n",
      "[16 / 20]   Val: Loss = 0.18567, Accuracy = 93.41%: 100%|██████████| 13/13 [00:07<00:00,  1.65it/s]\n",
      "[17 / 20] Train: Loss = 0.15807, Accuracy = 94.19%: 100%|██████████| 572/572 [00:54<00:00, 10.50it/s]\n",
      "[17 / 20]   Val: Loss = 0.18673, Accuracy = 93.33%: 100%|██████████| 13/13 [00:07<00:00,  1.69it/s]\n",
      "[18 / 20] Train: Loss = 0.15667, Accuracy = 94.24%: 100%|██████████| 572/572 [00:54<00:00, 10.47it/s]\n",
      "[18 / 20]   Val: Loss = 0.18421, Accuracy = 93.42%: 100%|██████████| 13/13 [00:07<00:00,  1.81it/s]\n",
      "[19 / 20] Train: Loss = 0.15567, Accuracy = 94.25%: 100%|██████████| 572/572 [00:54<00:00, 10.49it/s]\n",
      "[19 / 20]   Val: Loss = 0.18435, Accuracy = 93.43%: 100%|██████████| 13/13 [00:07<00:00,  1.70it/s]\n",
      "[20 / 20] Train: Loss = 0.15457, Accuracy = 94.26%: 100%|██████████| 572/572 [00:54<00:00, 10.50it/s]\n",
      "[20 / 20]   Val: Loss = 0.18326, Accuracy = 93.43%: 100%|██████████| 13/13 [00:07<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 8s, sys: 2min 38s, total: 20min 46s\n",
      "Wall time: 20min 29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=20,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ne_8f24h8kg"
   },
   "source": [
    "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
    "\n",
    "Добейтесь качества лучше прошлых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T16:19:05.683120Z",
     "start_time": "2019-09-07T16:19:05.679986Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data, batch_size):\n",
    "    \n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    model.eval() # evaluation mode\n",
    "    \n",
    "    for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                \n",
    "        X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "        logits = model(X_batch)\n",
    "\n",
    "        cur_correct_count, cur_sum_count = calc_accuracy_pad(y_batch, logits)\n",
    "        #cur_correct_count, cur_sum_count = calc_accuracy(y_batch, logits)\n",
    "        \n",
    "        correct_count += cur_correct_count\n",
    "        sum_count += cur_sum_count\n",
    "    \n",
    "    return float(correct_count/sum_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T16:19:16.245106Z",
     "start_time": "2019-09-07T16:19:07.490698Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "HPUuAPGhEGVR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9342752582891806\n",
      "CPU times: user 7.5 s, sys: 1.26 s, total: 8.76 s\n",
      "Wall time: 8.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# calc_accuracy_pad\n",
    "res = evaluate_model(model, (X_test, y_test), batch_size=32)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 06 - RNNs, part 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
